% IEEE conference paper describing the Research Platform project
\documentclass[conference]{IEEEtran}

% Packages
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{url}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}

% Hyphenation fixes
\hyphenation{op-tical net-works semi-conduc-tor}

% Metadata (edit authors/affiliations as appropriate)
\title{An Extensible Research Collaboration Platform with Integrated Search, Paper Management, and ML-powered Assistance}

\author{%
\IEEEauthorblockN{Somisetti Sridhar}
\IEEEauthorblockA{\small SCOPE\\
\small (VIT-AP University)\\
\small Amaravathi, Andhra Pradesh, India}
\and
\IEEEauthorblockN{Barma Ram Charan}
\IEEEauthorblockA{\small SCOPE\\
\small (VIT-AP University)\\
\small Amaravati, Andhra Pradesh, India}
\and
\IEEEauthorblockN{Maruri Sai Rama Linga Reddy}
\IEEEauthorblockA{\small SCOPE\\
\small (VIT-AP University)\\
\small Amaravati, Andhra Pradesh, India}}

\begin{document}

\maketitle

\begin{abstract}
We present a modular research collaboration platform that unifies scholarly paper ingestion and management, SQL-filtered search, basic recommendations, and real-time discussion within a production-grade Django application. The stack includes Django and Django Channels, a Celery scaffold, and a minimal rule-based chatbot. Implemented features comprise user and group workspaces, searchable paper repositories, bookmarking, ratings, citation tracking, a role-based publishing workflow (publisher/moderator/admin), WebSocket-based chat rooms, and read-only API endpoints for papers, bookmarks, and ratings. We document the concrete architecture and behavior as implemented in the codebase, avoiding speculative components.\footnote{All features described here are present in the repository at submission time.}
\end{abstract}

\begin{IEEEkeywords}
Research platforms, scholarly communication, information retrieval, recommender systems, natural language processing, Django, Celery, WebSockets
\end{IEEEkeywords}

\section{Introduction}
Research teams increasingly require end-to-end software infrastructure to discover, curate, and collaborate around scholarly content. Ad-hoc toolchains---disparate PDF folders, messaging tools, and standalone notebooks---impede reproducibility and knowledge retention. To address this gap, we built a modular, extensible research platform that consolidates paper management, advanced search, recommendation, summarization, and team collaboration into a single web application.

Our contributions are threefold:
\begin{itemize}
  \item A cohesive, production-ready architecture that couples a robust Django backend with asynchronous workers (Celery) and real-time communications (Django Channels) to support multi-user collaboration and background ML tasks.
  \item Pluggable ML services for paper summarization, recommendation, and conversational assistance, with facilities to schedule, cache, and monitor long-running jobs.
  \item A domain model and API surface enabling groups, role-based permissions, approval workflows, and paper-centric activities (uploading, annotating, bookmarking, and discussing), designed for maintainability and future extensibility.
\end{itemize}

\noindent\textit{Paper Organization}---Section~II surveys the system; Section~III details architecture and modules; Section~IV describes the data model and API; Section~V summarizes implemented ML-related utilities; Section~VI discusses background processing; Section~VII covers security and privacy; Section~VIII describes user experience; Section~IX outlines functional evaluation; Section~X discusses deployment; Section~XI reviews related work; Section~XII concludes with limitations and future work.

\section{System Overview}
The platform follows a service-oriented modularization within a single Django project tree. Figure~\ref{fig:modules} (conceptual) outlines six primary application modules: \emph{accounts}, \emph{papers}, \emph{search}, \emph{chat}, \emph{groups}, and \emph{ml\_engine}. Cross-cutting concerns include authentication and permissions, background processing, logging, and templates/static assets for the UI.

\subsection{Core Technologies}
\begin{itemize}
  \item \textbf{Web framework}: Django (views, models, serializers/templates, URL routing) with Django Channels for WebSockets.
  \item \textbf{Task orchestration}: Celery for asynchronous jobs (e.g., model inference, scheduled refreshes).
  \item \textbf{Data store}: SQLite for development; the schema is migration-managed and portable to production RDBMS.
  \item \textbf{ML stack}: Python-based NLP pipelines (e.g., BART summarization), with serialized models under \texttt{ml\_engine/ml\_models/}.
\end{itemize}

\subsection{End-to-End Workflow}
\textbf{Paper lifecycle}: A user with the \texttt{publisher} role uploads a paper via \texttt{PaperUploadView}; moderators/admins can directly approve on upload; otherwise, the paper enters a \textit{pending approval} state and becomes visible after a moderator/admin action in \texttt{PendingApprovalView}. Approved papers appear in lists and are searchable.

\textbf{Discovery and curation}: Users search approved papers using keyword filters and facets (category, author, year range). From a paper's detail page, users can bookmark, rate with a textual review, and view citations where the paper is cited or which papers it cites.

\textbf{Collaboration}: Each paper has an associated chat room (created on first access) for synchronous discussion; group-level chat rooms enable broader collaboration contexts. A simple \texttt{@bot} trigger returns factoid responses grounded in the paper's fields.

\textbf{Recommendations}: When recommendation entries are present for a user, a dedicated view renders recommended papers from \texttt{UserRecommendation}. Population of those entries is done via the prototype engine.

\section{Architecture and Modules}
\subsection{Accounts}
The \texttt{apps/accounts} module manages user registration/authentication, profiles, and role-based access controls. Admin dashboards expose user management and audit utilities. Forms and serializers encapsulate validation and data transfer for account operations.

\subsection{Groups}
\texttt{apps/groups} organizes collaborative spaces. Users may create or join groups, manage membership, and configure permissions for paper access and chat rooms. Group pages aggregate shared resources and activity streams.

\subsection{Papers}
The \texttt{apps/papers} module provides ingestion (upload), metadata editing, categorization, bookmarking, and approval workflows. Paper assets (PDFs) are stored under \texttt{media/papers/pdfs} and referenced by models. Views and templates implement list/detail pages, edit forms, recommendations, summaries, and personalized feeds (e.g., \texttt{my\_papers}). Signals support side effects (e.g., computing summaries on upload), while utilities handle parsing, validation, and background job submission.

\subsection{Search}
\texttt{apps/search} exposes search and history views. While the default configuration targets the relational database, the design accommodates IR backends (e.g., vector search or full-text indexes). Documents are normalized via \texttt{documents.py} to decouple upstream storage from search presentation.

\subsection{Chat}
\texttt{apps/chat} implements synchronous collaboration using Django Channels. \texttt{consumers.py} defines WebSocket consumers for one-to-one and group chat rooms. Real-time messaging complements paper-centric workflows (e.g., discussing a paper in context). The module stores rooms, messages, and membership; templates include room UIs and chat lists.

\subsection{ML Engine}
\texttt{apps/ml\_engine} currently provides: (i) a simple rule-based chatbot utility (\texttt{chatbot.py}) that answers basic questions about a paper's fields (authors, abstract, date, categories); (ii) data models for storing embeddings and user recommendations; and (iii) a recommendation engine prototype. The chatbot is minimal and deterministic; recommendations rely on stored ratings/bookmarks and (optionally) text representations. At present, the repository contains code to compute and store embeddings and user recommendations, but invocation is not wired into user flows by default.

\subsection{Cross-Cutting Services}
Common services include structured logging, centralized error handling, and permission middleware. Templates under \texttt{templates/} and assets under \texttt{static/} enforce consistent UI patterns. Configuration toggles switch development/production behaviors (debug flags, static serving, broker endpoints).

\section{Data Model and API}
The domain model captures Users, Groups, Papers, and Chat entities. Papers include metadata (title, authors, abstract, categories), file paths, and status flags (e.g., submitted, pending approval, published). Groups relate users and papers through membership and sharing policies. Chat rooms reference users and groups, supporting both ad-hoc and group-based discussions.

\subsection{Permissions Model}
Role strings (e.g., \texttt{publisher}, \texttt{moderator}, \texttt{admin}) govern access in views:\\
\textit{Upload}: publisher/moderator/admin; \textit{Approve/Reject}: moderator/admin; \textit{Edit/Delete}: owner (publisher) or moderator/admin; \textit{View pending}: moderator/admin; \textit{Download}: approved papers only. These checks are enforced using view \texttt{dispatch()} guards and mixins.

\subsection{Relational Schema}
Core tables include \texttt{User}, \texttt{Profile}, \texttt{Group}, \texttt{Membership}, \texttt{Paper}, \texttt{Category}, \texttt{Bookmark}, \texttt{ChatRoom}, and \texttt{Message}. Foreign keys enforce referential integrity (e.g., \texttt{Paper.author \textrightarrow{} User}, \texttt{Message.room \textrightarrow{} ChatRoom}). Indices on \texttt{Paper(title)}, \texttt{Paper(categories)}, and \texttt{Message(created\_at)} support fast retrieval.

\subsection{API Design}
Server-rendered views are primary. Additionally, read-only REST endpoints implemented via Django REST Framework expose listings for papers, bookmarks, and ratings; creation endpoints are stubbed and return 501 by design. Authorization uses session authentication and the same role checks as server views.

\subsection{Data Paths and Media}
Uploaded PDFs are stored under \texttt{media/papers/pdfs}; avatars under \texttt{media/avatars}. Paths are referenced from \texttt{Paper} fields. Downloads are permitted only for approved papers, and download counters are maintained at request time.

\section{Machine Learning Components}
\subsection{Summarization (BART + LoRA)}
The repository includes an executable summarization pipeline in \texttt{ml\_models/bart\_summarizer.py} that uses a base BART model (\texttt{facebook/bart-base}) with LoRA fine-tuning via the PEFT adapter. The class \texttt{BARTSummarizer} loads a tokenizer and composes the base model with LoRA weights using \texttt{PeftModel.from\_pretrained}. Device selection prefers Apple MPS then CUDA, falling back to CPU. Inference is run in evaluation mode without gradient tracking.

\paragraph*{Text-to-Summary Generation}
Given input text, the pipeline tokenizes with a 1024-token cap and generates summaries using beam search (default 4 beams), with configurable \texttt{max\_length}, \texttt{min\_length}, and \texttt{length\_penalty}. Sampling is disabled (\texttt{do\_sample=False}) for determinism. Outputs are detokenized with special tokens removed and whitespace normalized.

\paragraph*{Hierarchical Summarization}
Long documents are split into chunks (via a helper PDF extractor), summarized individually to \(\approx\)128 tokens, concatenated, and re-summarized to a final target length (default 256). If the concatenated intermediate summary remains long, secondary chunking and recursive summarization are performed. This two-stage strategy reduces exposure to sequence truncation while preserving salient content.

\paragraph*{PDF Pipeline}
The CLI entrypoint (\texttt{\_\_main\_\_}) invokes \texttt{summarize\_text\_from\_pdf}, which extracts and chunks text, constructs the summarizer pointing to a local LoRA output directory, and runs hierarchical summarization, logging progress and handling errors robustly.

\paragraph*{Operational Notes}
Model and tokenizer are cached across calls within the class. Device placement is explicit (MPS/CUDA/CPU). The LoRA adapter enables lightweight domain adaptation without shipping a full fine-tuned model checkpoint.

\subsection{Summarizer Training and Evaluation Metrics}
We report the metrics collected during fine-tuning and evaluation of the summarizer.

\subsubsection{Quality Metrics}
Table~\ref{tab:sum-metrics} summarizes overlap-based and semantic metrics. ROUGE-1/2/L indicate unigram, bigram, and longest common subsequence overlap; BLEU measures n-gram precision; METEOR accounts for synonymy and stemming; BERTScore F1 approximates semantic similarity.

\begin{table}[t]
\centering
\caption{Summarization quality metrics}
\begin{tabular}{lcc}
\toprule
Metric & Value & Interpretation \\
\midrule
ROUGE-1 & 0.364 & Moderate unigram overlap \\
ROUGE-2 & 0.216 & Decent phrase-level similarity \\
ROUGE-L & 0.299 & Moderate sentence-level matching \\
BLEU & 0.219 & Fair n-gram precision \\
METEOR & 0.313 & Reasonable for summarization \\
BERTScore F1 & $\sim$0.4--0.8 & Partial to good semantic match \\
\bottomrule
\end{tabular}
\label{tab:sum-metrics}
\end{table}

\subsubsection{Training Dynamics}
Table~\ref{tab:loss} lists representative training and eval losses by epoch fraction; the average training loss across reported steps is 5.9585. Eval loss decreases from 5.97 to 4.22, indicating improved generalization during early epochs.

\begin{table}[t]
\centering
\caption{Epoch-wise losses (excerpt)}
\begin{tabular}{lcc}
\toprule
Epoch & Training Loss & Eval Loss \\
\midrule
0.56 & 7.3344 & 5.9724 \\
1.11 & 5.2406 & 4.4543 \\
1.67 & 4.9935 & 4.2861 \\
2.22 & 4.9037 & 4.2395 \\
2.78 & 4.9108 & 4.2177 \\
\midrule
Avg. Train & 5.9585 & -- \\
\bottomrule
\end{tabular}
\label{tab:loss}
\end{table}

\subsection{Recommendation}
The platform provides a prototype recommendation engine and persistent storage for per-paper embeddings and per-user recommendations. The \texttt{papers:get\_recommendations} view reads \texttt{UserRecommendation} entries to render recommendations when available. Generating recommendations requires running the prototype engine out-of-band.

\subsubsection{Implemented Engine}
The class \texttt{ImprovedRecommendationEngine} encodes each paper using \texttt{all-MiniLM-L6-v2} sentence embeddings and stores vectors in \texttt{PaperEmbedding}. A user's profile vector is computed as the mean of embeddings for highly rated (\(\ge 4\)) and bookmarked papers. Content similarity uses cosine:
\[ s_{\mathrm{cos}}(u, i) = \frac{\mathbf{e}_u \cdot \mathbf{e}_i}{\lVert \mathbf{e}_u \rVert \, \lVert \mathbf{e}_i \rVert} \]
where \(\mathbf{e}_u\) is the user vector and \(\mathbf{e}_i\) is the paper vector. Collaborative signals are derived from co-preference counts over \texttt{Rating}. A hybrid score combines content, collaborative, and popularity signals with weights \(\alpha, \beta, \gamma\) and min-max normalization.

\subsubsection{Storage and Serving}
Embeddings are persisted in \texttt{PaperEmbedding} (JSON vectors with a \texttt{model\_version} tag). Recommendations are written to \texttt{UserRecommendation} with a human-readable reason string. The papers view \texttt{get\_recommendations} retrieves top entries for the current user and renders them in HTML or JSON.

\subsubsection{Operational Notes}
Embedding construction is implemented as an explicit method and intended to be scheduled; Celery task stubs reference an earlier \texttt{RecommendationEngine} name and require alignment with the current implementation for automation.

\subsubsection{Complexity and Resources}
Embedding build is \(\mathcal{O}(N d)\) for \(N\) papers and embedding dimension \(d\) (constant for \texttt{all-MiniLM-L6-v2}). Similarity scoring over \(M\) candidate papers is \(\mathcal{O}(M d)\). Collaborative aggregation operates on ratings with simple counting and filtering. The implementation targets CPU inference and stores embeddings as JSON arrays.

\subsection{Conversational Assistance}
Two simple pathways exist: (i) a WebSocket bot trigger in chat rooms using the \texttt{@bot} prefix, which returns a canned acknowledgement; and (ii) a minimal paper-aware chatbot utility that answers factoid queries from a paper object (authors, abstract snippet, publication date, categories). No neural conversational model is integrated.

\subsection{Hate Speech Detection for Chat Moderation}
The repository includes a moderation pipeline in \texttt{apps/chat/utils.py} designed to classify messages into \{hate, offensive, neutral\} using a saved Keras model (\texttt{hate\_speech\_detection.keras}) and tokenizer (\texttt{tokenizer.pkl}). The implementation covers preprocessing (lowercasing, punctuation removal, stopword filtering with an exception list, lemmatization), tokenization, and padding to a fixed length prior to model inference. Functions are provided for batch and single-message predictions with optional confidence thresholds and boolean convenience wrappers (\texttt{is\_offensive}).

At present, the moderation code is commented out and the active \texttt{is\_offensive} stub returns \texttt{False}, meaning moderation is disabled by default while the model artifacts are present on disk. Enabling moderation requires uncommenting the loader, ensuring NLTK resources are available, and wiring inference on message receipt paths.

\subsubsection{Preprocessing Pipeline}
The code normalizes to lowercase, strips punctuation, removes stopwords except an essential list (e.g., \texttt{"i", "you", "love", "hate", ...}), and lemmatizes tokens before tokenization via the saved Keras tokenizer. Sequences are padded/truncated to the fixed \texttt{MAX\_LEN} used in training prior to model inference.

\subsubsection{Classification Interface}
Batch predictions return string classes with an optional confidence threshold, mapping argmax probabilities to labels (\{hate, offensive, neutral\}) and yielding \texttt{"uncertain"} if below threshold. Boolean helpers expose per-message block decisions.

\subsubsection{Evaluation Metrics}
We evaluated the tri-class classifier (hate/offensive/neutral) and obtained the following classification report (precision/recall/F1/support) and confusion matrix. These values reflect class imbalance typical in moderation datasets, with high performance on \textit{offensive}, strong performance on \textit{neutral}, and comparatively weaker performance on the rarer \textit{hate} class.

\begin{table}[t]
\centering
\caption{Classification report for hate speech model}
\begin{tabular}{lcccc}
\toprule
Class & Precision & Recall & F1 & Support \\
\midrule
hate & 0.32 & 0.37 & 0.34 & 286 \\
offensive & 0.93 & 0.91 & 0.92 & 3838 \\
neutral & 0.79 & 0.81 & 0.80 & 833 \\
\midrule
accuracy & \multicolumn{3}{c}{0.86} & 4957 \\
macro avg & 0.68 & 0.70 & 0.69 & 4957 \\
weighted avg & 0.87 & 0.86 & 0.86 & 4957 \\
\bottomrule
\end{tabular}
\label{tab:hate-report}
\end{table}

\begin{table}[t]
\centering
\caption{Confusion matrix (rows=true labels, cols=predictions)}
\begin{tabular}{lccc}
\toprule
 & hate & offensive & neutral \\
\midrule
hate & 105 & 153 & 28 \\
offensive & 195 & 3487 & 156 \\
neutral & 30 & 125 & 678 \\
\bottomrule
\end{tabular}
\label{tab:hate-conf}
\end{table}

\subsection{Model Management}
The codebase includes database models for embeddings and recommendations. There is no automated model lifecycle orchestration enabled by default.

\section{Background Processing and Scheduling}
Celery is scaffolded and configured; a debug task is registered. Task stubs for paper processing and recommendation generation exist but depend on a prototype engine and are not decorated for automatic discovery. Logging under \texttt{logs/django.log} aids observability.

\subsection{Reliability}
Given the current scaffold, reliability considerations are limited to request/response paths in views and WebSocket consumers. Production deployments should add retry policies and task idempotency when wiring background jobs.

\section{Security and Privacy}
Role-based permissions guard actions across modules. Sensitive files (e.g., PDFs, avatars) are served via controlled views or secured storage. Authentication leverages Django's session framework; CSRF protection secures forms. For deployments handling proprietary papers, encrypted storage and audit trails are recommended.

\subsection{Threat Model}
We consider network adversaries and curious insiders. Controls include least-privilege roles, server-side authorization checks, CSRF/XSS protections, rate limiting, and secure cookies. For regulated environments, at-rest encryption and centralized secret management are advised.

\subsection{Privacy}
Personally identifiable information is minimized. Access logs avoid content payloads unless explicitly enabled for debugging. Chat retention policies are configurable per group.

\section{User Experience}
The UI, implemented via Django templates and static assets, surfaces: dashboards (per-user and admin), paper lists and details, recommendations (when present), summaries, search/history, group pages, and chat rooms. Templates located under \texttt{templates/} provide cohesive navigation and consistent affordances.

\subsection{Accessibility}
Templates follow semantic HTML with ARIA roles where applicable. Text alternatives are provided for media; color choices meet contrast guidelines.

\subsection{Templates and Views}
Key templates include list/detail views for papers (lists, categories, bookmarks, pending approval, admin list), search results and advanced search, chat rooms for paper and group contexts, and dashboards. Context variables include search queries, selected facets, ratings, citations, and pagination controls. Views populate these via queryset methods and \texttt{get\_context\_data} overrides.

\section{Functional Evaluation}
We verify implemented behaviors via the following user-visible checks:
\begin{itemize}
  \item Paper lifecycle: upload (publisher), approval (moderator/admin), edit/delete (role-gated), detail view counters (views/downloads).
  \item Search: keyword filtering by title/abstract/authors; faceting by category, author, and year bounds; search history for authenticated users.
  \item Social features: bookmarks CRUD, ratings with text reviews, citation lists.
  \item Groups: listing and membership-driven access to shared resources.
  \item Chat: real-time messaging in rooms; \texttt{@bot} acknowledgement path; optional moderation hook (currently disabled by default).
  \item API: read-only JSON listings for papers, bookmarks, and ratings; 501 on create.
\end{itemize}
These checks are implemented and exercised through views, templates, and consumers in the repository.

\subsection{ML Sanity Checks}
For recommendations, we validate that (i) embeddings are created for approved papers, (ii) a user profile vector is computed from ratings/bookmarks, and (iii) \texttt{UserRecommendation} entries are produced with descending scores. For moderation, we validate model and tokenizer loading paths and tokenization/padding shapes; we do not report accuracy metrics here as evaluation scripts and logs are not included in the repository.

\subsection{Channels/WebSocket Protocol}
The consumer joins a \texttt{chat\_\{room\_id\}} group on connect, persists incoming messages, and broadcasts JSON payloads containing message text, user, timestamp, and an \texttt{is\_bot} flag. The server emits a bot response when the \texttt{@bot} prefix is present. Disconnect removes the channel from the group.

\section{Deployment and Operations}
The application ships with a virtual environment and migration-managed database. Channels/Redis are configured for WebSockets; Celery is configured but requires task decoration to auto-discover jobs. For production, use an ASGI stack (e.g., Daphne/Uvicorn), a production RDBMS, and object storage for media.

\subsection{Configuration}
Settings include installed apps for REST framework and Channels, Redis channel layer configuration, and media/static roots. The Celery application is initialized in \texttt{research\_platform/celery.py} and auto-discovers tasks from installed apps. URL routing defines app namespaces for \texttt{accounts}, \texttt{papers}, \texttt{search}, \texttt{chat}, \texttt{groups}, and \texttt{ml\_engine}.

\subsection{Operational Guidance}
\textit{Recommendation pipeline}: run embedding build and hybrid ranking periodically and persist to \texttt{UserRecommendation}.\newline
\textit{Moderation}: ensure model/tokenizer are deployed, uncomment the pipeline, and add resource downloads for NLTK.\newline
\textit{Chat scale-out}: configure Channels Redis and run multiple ASGI workers; persistence of messages is handled by the DB models.

\subsection{Scalability}
Horizontal scaling adds web workers and Celery workers. Channels layers (e.g., Redis) support chat fan-out. Static/media assets should be served via a CDN; long-running ML tasks are isolated in worker pools with resource quotas.

\subsection{Observability}
Structured logs, metrics (request latency, task throughput), tracing, and health checks enable diagnosis and integration with orchestrators.

\section{Related Work}
Our platform intersects scholarly IR \cite{manning2008ir}, summarization \cite{lewis2019bart}, and conversational systems \cite{roller2021recipes}. Systems like Zotero and Mendeley focus on citation management, while research knowledge bases emphasize search. We integrate these capabilities with group collaboration and real-time chat, emphasizing extensibility within a single coherent web application.

\section{Limitations and Ethical Considerations}
Current limitations include: (i) search is purely SQL-based without full-text indexing; (ii) recommendation generation is present as a prototype and not wired to user flows; (iii) chatbot responses are rule-based. For private corpora, deployments should enforce strict access control and audit logging.

\section{Conclusion}
We described a modular research platform unifying paper management, search, recommendations, and chat within a production-ready Django architecture. Future work includes integration with external IR backends (e.g., scalable vector search), fine-tuned domain models for summarization/recommendation, richer conversational grounding, and expanded analytics on collaboration dynamics. We release the architectural template to accelerate institutional deployments.

\section*{Reproducibility Checklist}
We provide: (i) dependency specifications (\texttt{requirements.txt}); (ii) migration-managed schemas; (iii) a Channels configuration for WebSockets; (iv) a Celery configuration scaffold; and (v) templates and views corresponding to all user-visible features described above.

\section*{Acknowledgments}
We thank contributors and users for feedback on early prototypes and deployments.

\begin{thebibliography}{99}
\bibitem{manning2008ir}
C. D. Manning, P. Raghavan, and H. Sch"utze, ``Introduction to Information Retrieval,'' Cambridge University Press, 2008.

\bibitem{lewis2019bart}
M. Lewis, et al., ``BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension,'' in \emph{Proc. ACL}, 2020.

\bibitem{roller2021recipes}
S. Roller, et al., ``Recipes for Building an Open-Domain Chatbot,'' in \emph{Proc. EACL}, 2021.
 
\bibitem{vaswani2017attention}
A. Vaswani, et al., ``Attention Is All You Need,'' in \emph{NeurIPS}, 2017.

\bibitem{goodfellow2016deep}
I. Goodfellow, Y. Bengio, and A. Courville, ``Deep Learning,'' MIT Press, 2016.

\bibitem{hu2021lora}
E. Hu, et al., ``LoRA: Low-Rank Adaptation of Large Language Models,'' in \emph{ICLR}, 2022.

\bibitem{peft2022}
Y. Mangrulkar, et al., ``PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods,'' 2022. [Online]. Available: \url{https://github.com/huggingface/peft}

\bibitem{reimers2019sbert}
N. Reimers and I. Gurevych, ``Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,'' in \emph{EMNLP}, 2019.

\bibitem{thakur2021sentence}
N. Thakur, et al., ``Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks,'' in \emph{NAACL}, 2021.

\bibitem{wolf2020transformers}
T. Wolf, et al., ``Transformers: State-of-the-Art Natural Language Processing,'' in \emph{EMNLP}, 2020.

\bibitem{paszke2019pytorch}
A. Paszke, et al., ``PyTorch: An Imperative Style, High-Performance Deep Learning Library,'' in \emph{NeurIPS}, 2019.

\bibitem{chollet2015keras}
F. Chollet, ``Keras,'' 2015. [Online]. Available: \url{https://keras.io}

\bibitem{davidson2017hate}
T. Davidson, D. Warmsley, M. Macy, and I. Weber, ``Automated Hate Speech Detection and the Problem of Offensive Language,'' in \emph{ICWSM}, 2017.

\bibitem{django}
Django Software Foundation, ``Django Web Framework,'' [Online]. Available: \url{https://www.djangoproject.com/}

\bibitem{djangochannels}
``Django Channels,'' [Online]. Available: \url{https://channels.readthedocs.io/}

\bibitem{celery}
``Celery Distributed Task Queue,'' [Online]. Available: \url{https://docs.celeryq.dev/}

\bibitem{drf}
T. Christie, ``Django REST Framework,'' [Online]. Available: \url{https://www.django-rest-framework.org/}
\end{thebibliography}

% Figure placeholder (optional; add an image file to include)
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.9\linewidth]{images/architecture}
%   \caption{High-level module diagram of the platform.}
%   \label{fig:modules}
% \end{figure}

\end{document}